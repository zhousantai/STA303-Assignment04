{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 01: Multi-class Classification \n",
    "In this Assignment, you will train a deep model on the CIFAR10 from the scratch using PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:28:57.681073Z",
     "iopub.status.busy": "2024-01-13T05:28:57.680771Z",
     "iopub.status.idle": "2024-01-13T05:28:59.371975Z",
     "shell.execute_reply": "2024-01-13T05:28:59.371429Z",
     "shell.execute_reply.started": "2024-01-13T05:28:57.681035Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:28:59.373304Z",
     "iopub.status.busy": "2024-01-13T05:28:59.373089Z",
     "iopub.status.idle": "2024-01-13T05:28:59.376761Z",
     "shell.execute_reply": "2024-01-13T05:28:59.376142Z",
     "shell.execute_reply.started": "2024-01-13T05:28:59.373286Z"
    }
   },
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1 \n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 60\n",
    "\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:28:59.377446Z",
     "iopub.status.busy": "2024-01-13T05:28:59.377299Z",
     "iopub.status.idle": "2024-01-13T05:28:59.459973Z",
     "shell.execute_reply": "2024-01-13T05:28:59.459533Z",
     "shell.execute_reply.started": "2024-01-13T05:28:59.377430Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:28:59.460683Z",
     "iopub.status.busy": "2024-01-13T05:28:59.460530Z",
     "iopub.status.idle": "2024-01-13T05:29:16.308079Z",
     "shell.execute_reply": "2024-01-13T05:29:16.307550Z",
     "shell.execute_reply.started": "2024-01-13T05:28:59.460667Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 26421880/26421880 [00:10<00:00, 2604562.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29515/29515 [00:00<00:00, 180279.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/train-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4422102/4422102 [00:02<00:00, 1782397.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-images-idx3-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5148/5148 [00:00<00:00, 16790262.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/FashionMNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/FashionMNIST/raw\n",
      "\n",
      "['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def convert_to_rgb(x):\n",
    "    return x.repeat(3, 1, 1)\n",
    "\n",
    "\n",
    "\n",
    "transform_mnist_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(convert_to_rgb), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_mnist_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(convert_to_rgb), \n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "train_set = torchvision.datasets.FashionMNIST(root='../data', train=True,\n",
    "                                              download=True, transform=transform_mnist_train)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True, num_workers=2)\n",
    "\n",
    "test_set = torchvision.datasets.FashionMNIST(root='../data', train=False,\n",
    "                                             download=True, transform=transform_mnist_test)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                              shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = train_set.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.309016Z",
     "iopub.status.busy": "2024-01-13T05:29:16.308811Z",
     "iopub.status.idle": "2024-01-13T05:29:16.314551Z",
     "shell.execute_reply": "2024-01-13T05:29:16.314175Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.308996Z"
    }
   },
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        # 第一层卷积层，3个输入通道（对于RGB图像），32个输出通道，3x3卷积核\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        # 第二层卷积层\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        # 第三层卷积层\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        # 最大池化层\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        # Dropout层\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        # 全连接层\n",
    "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
    "        self.fc2 = nn.Linear(256, 10) # 10个输出类别\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 通过卷积层、批量归一化层、激活函数、池化层和dropout层\n",
    "        x = self.pool(F.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(F.relu(self.bn3(self.conv3(x))))\n",
    "        # 扁平化特征图\n",
    "        x = x.view(-1, 128 * 3 * 3)\n",
    "        # 通过dropout层和全连接层\n",
    "        x = self.dropout(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.315355Z",
     "iopub.status.busy": "2024-01-13T05:29:16.315065Z",
     "iopub.status.idle": "2024-01-13T05:29:16.517844Z",
     "shell.execute_reply": "2024-01-13T05:29:16.517279Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.315338Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1152, out_features=256, bias=True)\n",
       "  (fc2): Linear(in_features=256, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.519943Z",
     "iopub.status.busy": "2024-01-13T05:29:16.519745Z",
     "iopub.status.idle": "2024-01-13T05:29:16.523909Z",
     "shell.execute_reply": "2024-01-13T05:29:16.523400Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.519925Z"
    }
   },
   "outputs": [],
   "source": [
    "#optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=STEP, gamma=GAMMA)\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# 定义warm-up学习率调度器\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, multiplier):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (multiplier - 1.0) * epoch / warmup_epochs + 1.0\n",
    "        return 1.0\n",
    "\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 在训练代码中引入warm-up学习率调度器\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "warmup_epochs = NUM_EPOCHS//5\n",
    "multiplier = 10\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, multiplier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: per batch training/testing\n",
    "---\n",
    "\n",
    "Please denfine two function named ``train_batch`` and ``test_batch``. These functions are essential for training and evaluating machine learning models using batched data from dataloaders.\n",
    "\n",
    "**To do**: \n",
    "1. Define the loss function i.e [nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
    "2. Take the image as the input and generate the output using the pre-defined SimpleNet.\n",
    "3. Calculate the loss between the output and the corresponding label using the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.524582Z",
     "iopub.status.busy": "2024-01-13T05:29:16.524415Z",
     "iopub.status.idle": "2024-01-13T05:29:16.526984Z",
     "shell.execute_reply": "2024-01-13T05:29:16.526624Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.524566Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "    output = model(image)\n",
    "    #print(\"Output size:\", output.size())  # Add this line to check the output size\n",
    "    loss = criterion(output, target)\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.527784Z",
     "iopub.status.busy": "2024-01-13T05:29:16.527480Z",
     "iopub.status.idle": "2024-01-13T05:29:16.530173Z",
     "shell.execute_reply": "2024-01-13T05:29:16.529753Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.527766Z"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def one_hot_encoding(target, num_classes):\n",
    "    return F.one_hot(target, num_classes=num_classes).float()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.530853Z",
     "iopub.status.busy": "2024-01-13T05:29:16.530684Z",
     "iopub.status.idle": "2024-01-13T05:29:16.533162Z",
     "shell.execute_reply": "2024-01-13T05:29:16.532741Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.530836Z"
    }
   },
   "outputs": [],
   "source": [
    "def test_batch(model, image, target):\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2024-01-13T05:29:16.533957Z",
     "iopub.status.busy": "2024-01-13T05:29:16.533778Z",
     "iopub.status.idle": "2024-01-13T05:29:16.538246Z",
     "shell.execute_reply": "2024-01-13T05:29:16.537693Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.533941Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOF while scanning triple-quoted string literal (3761160680.py, line 118)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[11], line 118\u001b[0;36m\u001b[0m\n\u001b[0;31m    print(f\"训练模型用时：{duration}秒\")\u001b[0m\n\u001b[0m                                \n^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOF while scanning triple-quoted string literal\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "training_loss = []\n",
    "training_acc = []\n",
    "testing_loss = []\n",
    "testing_acc = []\n",
    "start_time = time.time()  # 记录开始时间\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "# 日志文件\n",
    "log_file = 'training_log.txt'\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    ##########################\n",
    "    ### Training\n",
    "    ##########################\n",
    "\n",
    "    running_cls_loss = 0.0\n",
    "    running_cls_corrects = 0\n",
    "\n",
    "    for batch_idx, (image, target) in enumerate(train_dataloader):\n",
    "\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        \n",
    "        outputs, loss = train_batch(model, image, target)\n",
    "        #########################\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        \n",
    "        loss_data = loss.data.item()\n",
    "        if np.isnan(loss_data):\n",
    "            raise ValueError('loss is nan while training')\n",
    "        running_cls_loss += loss.item()\n",
    "        running_cls_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    epoch_loss = running_cls_loss / len(train_set)\n",
    "    epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "    print(f'Epoch: {epoch+1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "    \n",
    "    #########\n",
    "    all_preds.extend(preds.cpu().numpy())\n",
    "    all_targets.extend(target.cpu().numpy())\n",
    "    f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "    print(f\"f1 score in {epoch}th epoch is {f1}\")\n",
    "\n",
    "############\n",
    "    training_loss.append(epoch_loss)\n",
    "    training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "    # change learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "\n",
    "    ##########################\n",
    "    ### Testing\n",
    "    ##########################\n",
    "    # # eval model during training or in the last epoch\n",
    "    \n",
    "    if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch +1) == NUM_EPOCHS:\n",
    "        print('Begin test......')\n",
    "        model.eval()\n",
    "    \n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "\n",
    "        for batch_idx, (image, target) in enumerate(test_dataloader):\n",
    "\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            # test model\n",
    "            ##################\n",
    "          #  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "            outputs, loss = test_batch(model, image, target)\n",
    "          #  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "            #######################\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "        val_loss = val_loss / len(test_set)\n",
    "        val_acc = val_corrects.double() / len(test_set)\n",
    "        print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "        testing_loss.append(val_loss)\n",
    "        testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "    with open(log_file, 'a') as f:\n",
    "        f.write(f'Epoch {epoch+1}/{NUM_EPOCHS}\\n')\n",
    "        f.write(f'Train Accuracy: {epoch_acc:.4f}%\\n')\n",
    "        f.write(f'Test Accuracy: {val_acc:.4f}%\\n')\n",
    "        f.write('\\n')\n",
    "\n",
    "        # save the model in last epoch\n",
    "        if (epoch +1) == NUM_EPOCHS:\n",
    "            \n",
    "            state = {\n",
    "            'state_dict': model.state_dict(),\n",
    "            'acc': epoch_acc,\n",
    "            'epoch': (epoch+1),\n",
    "            }\n",
    "\n",
    "            # check the dir\n",
    "            if not os.path.exists(SAVE_DIR):\n",
    "                os.makedirs(SAVE_DIR)\n",
    "\n",
    "            # save the state\n",
    "            torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch+1))))\n",
    "            \n",
    "end_time = time.time()  # 记录结束时间\n",
    "duration = end_time - start_time  # 计算训练时间\n",
    "print(f\"训练模型用时：{duration}秒\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Instance inference\n",
    "---\n",
    "The task is to visualizes an image along with model prediction and class probabilities.\n",
    "\n",
    "**To do**: \n",
    "1. Calculate the prediction and the probabilities for each class.\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.538634Z",
     "iopub.status.idle": "2024-01-13T05:29:16.538831Z",
     "shell.execute_reply": "2024-01-13T05:29:16.538742Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.538731Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# inputs, classes = next(iter(test_dataloader))\n",
    "# input = inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.539314Z",
     "iopub.status.idle": "2024-01-13T05:29:16.539497Z",
     "shell.execute_reply": "2024-01-13T05:29:16.539411Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.539402Z"
    }
   },
   "outputs": [],
   "source": [
    "# ##################### Write your answer here ##################\n",
    "# # input: image, model\n",
    "# # outputs: predict_label, probabilities\n",
    "# # predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "# ###############################################################\n",
    "\n",
    "# input_tensor = input.unsqueeze(0).to(device)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_tensor)\n",
    "#     probabilities = F.softmax(output, dim=1)\n",
    "#     predict_label = torch.argmax(probabilities)\n",
    "# # predict_label = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.540359Z",
     "iopub.status.idle": "2024-01-13T05:29:16.540750Z",
     "shell.execute_reply": "2024-01-13T05:29:16.540468Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.540457Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# predicted_class = class_names[predict_label.item()]\n",
    "# predicted_probability = probabilities[0][predict_label.item()]\n",
    "# image = input.numpy().transpose((1, 2, 0))\n",
    "# plt.imshow(image)\n",
    "# plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}', \n",
    "#             color='white', backgroundcolor='black', fontsize=8)\n",
    "# plt.show()\n",
    "\n",
    "# # Print probabilities for each class\n",
    "# print('Print probabilities for each class:')\n",
    "# for i in range(len(class_names)):\n",
    "#     print(f'{class_names[i]}: {probabilities[0][i].item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.541602Z",
     "iopub.status.idle": "2024-01-13T05:29:16.541836Z",
     "shell.execute_reply": "2024-01-13T05:29:16.541741Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.541729Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install torchcp\n",
    "# from torchcp.classification.scores import THR, APS, SAPS, RAPS\n",
    "# from torchcp.classification.predictors import SplitPredictor, ClusterPredictor, ClassWisePredictor\n",
    "\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# log_name=\"FASHION_MNIST\"\n",
    "# model=torch.load(\"FASHION_MNIST_model\")\n",
    "# results = []\n",
    "# alphas = [0.05, 0.1, 0.15, 0.2]\n",
    "# saps_weights = [0.5, 1, 1.5, 2]\n",
    "# raps_penalties = [0.05, 0.1, 0.15, 0.2]\n",
    "# raps_kregs = [0, 1, 2, 5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "\n",
    "# all_predict_sets_list=[]\n",
    "# for alpha in alphas:\n",
    "# #     for weight in saps_weights:\n",
    "# #         for penalty in raps_penalties:\n",
    "# #             for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"alpha loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result + \"\\n\")\n",
    "\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     predict_set_str = [' '.join(map(str, sublist)) for sublist in all_predict_set]\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write('\\n'.join(predict_set_str) + \"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "# results = []\n",
    "# all_predict_sets_list=[]\n",
    "# # for alpha in alphas:\n",
    "# #     for weight in saps_weights:\n",
    "# #         for penalty in raps_penalties:\n",
    "# for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"kreg loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result + \"\\n\")\n",
    "\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     predict_set_str = [' '.join(map(str, sublist)) for sublist in all_predict_set]\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write('\\n'.join(predict_set_str) + \"\\n\")\n",
    "\n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "# results = []\n",
    "# all_predict_sets_list=[]\n",
    "# # for alpha in alphas:\n",
    "# for weight in saps_weights:\n",
    "# #         for penalty in raps_penalties:\n",
    "# #             for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"weight loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result + \"\\n\")\n",
    "\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     predict_set_str = [' '.join(map(str, sublist)) for sublist in all_predict_set]\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write('\\n'.join(predict_set_str) + \"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "# results = []\n",
    "# all_predict_sets_list=[]\n",
    "# # for alpha in alphas:\n",
    "# #     for weight in saps_weights:\n",
    "# for penalty in raps_penalties:\n",
    "# #             for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"penalty loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result + \"\\n\")\n",
    "\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     predict_set_str = [' '.join(map(str, sublist)) for sublist in all_predict_set]\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write('\\n'.join(predict_set_str) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.542393Z",
     "iopub.status.idle": "2024-01-13T05:29:16.542586Z",
     "shell.execute_reply": "2024-01-13T05:29:16.542498Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.542487Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate_and_log_results(model, alphas, weights, penalties, kregs, train_dataloader, test_dataloader, device, log_name):\n",
    "    results = []\n",
    "    all_predict_sets_list = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for weight in weights:\n",
    "            for penalty in penalties:\n",
    "                for kreg in kregs:\n",
    "                    score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "                    predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "                    for score_function in score_functions:\n",
    "                        for Predictor in predictors:\n",
    "                            predictor = Predictor(score_function=score_function, model=model)\n",
    "                            predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "                            all_predict_sets = []\n",
    "                            for batch in test_dataloader:\n",
    "                                images, _ = batch\n",
    "                                images = images.to(device)\n",
    "                                predict_set = predictor.predict(images)\n",
    "                                all_predict_sets.extend(predict_set)\n",
    "                            all_predict_sets_list.append(all_predict_sets)\n",
    "                            result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "                            result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "                                          f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "                                          f\"Predictor: {Predictor.__name__}, \"\n",
    "                                          f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "                                          f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "                                          f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "                                          f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "                                          f\"Average Size: {result_dict['Average_size']}\")\n",
    "                            results.append(result_str)\n",
    "\n",
    "    # Write results to file\n",
    "    with open(f'{log_name}_results_new.txt', 'a') as file:\n",
    "        for result in results:\n",
    "            file.write(\"***********\")\n",
    "            file.write(result + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.543219Z",
     "iopub.status.idle": "2024-01-13T05:29:16.543449Z",
     "shell.execute_reply": "2024-01-13T05:29:16.543360Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.543350Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = [0.05, 0.1, 0.15, 0.2]\n",
    "saps_weights = [0.5, 1, 1.5, 2]\n",
    "raps_penalties = [0.05, 0.1, 0.15, 0.2]\n",
    "raps_kregs = [0, 1, 2, 5]\n",
    "log_name=\"FASHION_MNIST\"\n",
    "model=torch.load(\"FASHION_MNIST_model\")\n",
    "evaluate_and_log_results(model, [0.1], [1], [0.1], raps_kregs, train_dataloader, test_dataloader, device, log_name)\n",
    "evaluate_and_log_results(model, [0.1], [1], raps_penalties, [0], train_dataloader, test_dataloader, device, log_name)\n",
    "evaluate_and_log_results(model, [0.1], saps_weights, [0.1], [0], train_dataloader, test_dataloader, device, log_name)\n",
    "evaluate_and_log_results(model, alphas, [1], [0.1], [0], train_dataloader, test_dataloader, device, log_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.544087Z",
     "iopub.status.idle": "2024-01-13T05:29:16.544352Z",
     "shell.execute_reply": "2024-01-13T05:29:16.544259Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.544243Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#torch.save(model,\"FASHION_MNIST_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.545204Z",
     "iopub.status.idle": "2024-01-13T05:29:16.545442Z",
     "shell.execute_reply": "2024-01-13T05:29:16.545349Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.545338Z"
    }
   },
   "outputs": [],
   "source": [
    "# for i in np.array(training_acc):\n",
    "#     print(f\"{i:.5f}\",end=',')\n",
    "# print(\"\\n\\n\")\n",
    "# for i in np.array(testing_acc):\n",
    "#     print(f\"{i:.5f}\",end=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-13T05:29:16.546110Z",
     "iopub.status.idle": "2024-01-13T05:29:16.546342Z",
     "shell.execute_reply": "2024-01-13T05:29:16.546248Z",
     "shell.execute_reply.started": "2024-01-13T05:29:16.546236Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# epochs = list(range(1, 301))\n",
    "# line1 = ax.plot(epochs, np.array(training_acc), label=\"train acc\")\n",
    "# line2 = ax.plot(epochs, np.array(testing_acc), label=\"test acc\")\n",
    "# ax.set_xlabel('epochs')\n",
    "# ax.set_ylabel('accuracy')\n",
    "# ax.grid(True)\n",
    "# ax.set_ylim(0, 1)\n",
    "# ax.legend()\n",
    "# plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
