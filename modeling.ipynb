{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T03:02:21.599809Z",
     "start_time": "2023-12-27T03:02:19.083080Z"
    },
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:21.591181Z",
     "iopub.status.busy": "2023-12-29T12:41:21.590661Z",
     "iopub.status.idle": "2023-12-29T12:41:26.520471Z",
     "shell.execute_reply": "2023-12-29T12:41:26.519387Z",
     "shell.execute_reply.started": "2023-12-29T12:41:21.591139Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "import os\n",
    "import time\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71235067b82a0b0e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T03:02:21.600327Z",
     "start_time": "2023-12-27T03:02:21.587946Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:26.522394Z",
     "iopub.status.busy": "2023-12-29T12:41:26.522038Z",
     "iopub.status.idle": "2023-12-29T12:41:26.743436Z",
     "shell.execute_reply": "2023-12-29T12:41:26.742797Z",
     "shell.execute_reply.started": "2023-12-29T12:41:26.522372Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# random seed\n",
    "SEED = 1\n",
    "NUM_CLASS = 10\n",
    "\n",
    "# Training\n",
    "BATCH_SIZE = 128\n",
    "NUM_EPOCHS = 60\n",
    "EVAL_INTERVAL=1\n",
    "SAVE_DIR = './log'\n",
    "\n",
    "# Optimizer\n",
    "LEARNING_RATE = 1e-3\n",
    "MOMENTUM = 0.9\n",
    "STEP=5\n",
    "GAMMA=0.5\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3e7ef70c0f1fa51e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T03:02:21.600838Z",
     "start_time": "2023-12-27T03:02:21.593642Z"
    },
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:26.744594Z",
     "iopub.status.busy": "2023-12-29T12:41:26.744204Z",
     "iopub.status.idle": "2023-12-29T12:41:27.015086Z",
     "shell.execute_reply": "2023-12-29T12:41:27.014223Z",
     "shell.execute_reply.started": "2023-12-29T12:41:26.744574Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConvNet(\n",
       "  (conv1): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu1): ReLU()\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu2): ReLU()\n",
       "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (conv3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (bn3): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu3): ReLU()\n",
       "  (fc1): Linear(in_features=1024, out_features=128, bias=True)\n",
       "  (dropout1): Dropout(p=0.5, inplace=False)\n",
       "  (relu4): ReLU()\n",
       "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.relu1 = nn.ReLU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.relu2 = nn.ReLU()\n",
    "\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "\n",
    "        self.fc1 = nn.Linear(64 * 4 * 4, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.relu4 = nn.ReLU()\n",
    "\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu3(self.bn3(self.conv3(x))))\n",
    "\n",
    "        x = x.view(-1, 64 * 4 * 4)\n",
    "\n",
    "        x = self.relu4(self.dropout1(self.fc1(x)))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = ConvNet()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ca82879d6477525",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T03:02:21.728394Z",
     "start_time": "2023-12-27T03:02:21.600747Z"
    },
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:27.016128Z",
     "iopub.status.busy": "2023-12-29T12:41:27.015969Z",
     "iopub.status.idle": "2023-12-29T12:41:27.020259Z",
     "shell.execute_reply": "2023-12-29T12:41:27.019641Z",
     "shell.execute_reply.started": "2023-12-29T12:41:27.016111Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.optim import lr_scheduler\n",
    "\n",
    "# 定义warm-up学习率调度器\n",
    "def warmup_lr_scheduler(optimizer, warmup_epochs, multiplier):\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return (multiplier - 1.0) * epoch / warmup_epochs + 1.0\n",
    "        return 1.0\n",
    "\n",
    "    return lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "\n",
    "# 在训练代码中引入warm-up学习率调度器\n",
    "optimizer = optim.SGD(model.parameters(), lr=LEARNING_RATE, momentum=MOMENTUM)\n",
    "warmup_epochs = NUM_EPOCHS//5\n",
    "multiplier = 10\n",
    "scheduler = warmup_lr_scheduler(optimizer, warmup_epochs, multiplier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77e5c87e5268500b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T03:02:23.707427Z",
     "start_time": "2023-12-27T03:02:21.730884Z"
    },
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:27.021221Z",
     "iopub.status.busy": "2023-12-29T12:41:27.021066Z",
     "iopub.status.idle": "2023-12-29T12:41:27.027224Z",
     "shell.execute_reply": "2023-12-29T12:41:27.026795Z",
     "shell.execute_reply.started": "2023-12-29T12:41:27.021204Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transform_cifar10_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "transform_cifar10_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "transform_cifar100_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "\n",
    "transform_cifar100_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5071, 0.4867, 0.4408), (0.2675, 0.2565, 0.2761)),\n",
    "])\n",
    "transform_fashion_mnist = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "transform_mnist = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307, 0.1307, 0.1307), (0.3081, 0.3081, 0.3081)),\n",
    "])\n",
    "\n",
    "\n",
    "# train_set = torchvision.datasets.CIFAR100(root='../data', train=True,\n",
    "#                                           download=True, transform=transform_cifar100_train)\n",
    "# test_set = torchvision.datasets.CIFAR100(root='../data', train=False,\n",
    "#                                          download=True, transform=transform_cifar100_test)\n",
    "\n",
    "\n",
    "\n",
    "# train_set = torchvision.datasets.CIFAR10(root='../data', train=True,\n",
    "#                                         download=True, transform=transform_cifar10_train)\n",
    "\n",
    "\n",
    "# test_set = torchvision.datasets.CIFAR10(root='../data', train=False,\n",
    "#                                        download=True, transform=transform_cifar10_test)\n",
    "\n",
    "\n",
    "\n",
    "# train_set = torchvision.datasets.FashionMNIST(root='../data', train=True,\n",
    "#                                               download=True, transform=transform_fashion_mnist)\n",
    "# test_set = torchvision.datasets.FashionMNIST(root='../data', train=False,\n",
    "#                                              download=True, transform=transform_fashion_mnist)\n",
    "\n",
    "\n",
    "# train_set = torchvision.datasets.MNIST(root='../data', train=True,\n",
    "#                                        download=True, transform=transform_mnist)\n",
    "# test_set = torchvision.datasets.MNIST(root='../data', train=False,\n",
    "#                                       download=True, transform=transform_mnist)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2772b2e-97a9-491c-8212-a70f2b5f5f37",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:27.028113Z",
     "iopub.status.busy": "2023-12-29T12:41:27.027969Z",
     "iopub.status.idle": "2023-12-29T12:41:43.813948Z",
     "shell.execute_reply": "2023-12-29T12:41:43.812854Z",
     "shell.execute_reply.started": "2023-12-29T12:41:27.028097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../data/cifar-10-python.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 170498071/170498071 [00:12<00:00, 13133860.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/cifar-10-python.tar.gz to ../data\n",
      "Files already downloaded and verified\n",
      "['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == 'CIFAR10':\n",
    "        transform_train, transform_test = transform_cifar10_train, transform_cifar10_test\n",
    "        Dataset = torchvision.datasets.CIFAR10\n",
    "    elif dataset_name == 'CIFAR100':\n",
    "        transform_train, transform_test = transform_cifar100_train, transform_cifar100_test\n",
    "        Dataset = torchvision.datasets.CIFAR100\n",
    "    elif dataset_name == 'FashionMNIST':\n",
    "        transform_train, transform_test = transform_fashion_mnist, transform_fashion_mnist\n",
    "        Dataset = torchvision.datasets.FashionMNIST\n",
    "    elif dataset_name == 'MNIST':\n",
    "        transform_train, transform_test = transform_mnist, transform_mnist\n",
    "        Dataset = torchvision.datasets.MNIST\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset\")\n",
    "\n",
    "    train_set = Dataset(root='../data', train=True, download=True, transform=transform_train)\n",
    "    test_set = Dataset(root='../data', train=False, download=True, transform=transform_test)\n",
    "    return train_set, test_set\n",
    "\n",
    "# 使用函数\n",
    "train_set, test_set = load_dataset('CIFAR10')  # 替换 'CIFAR10' 为其他数据集名称\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE,\n",
    "                                          shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE,\n",
    "                                         shuffle=False, num_workers=2)\n",
    "\n",
    "class_names = train_set.classes\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f6bca32a1b0a1245",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-12-27T03:02:23.711144Z",
     "start_time": "2023-12-27T03:02:23.705948Z"
    },
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:43.816223Z",
     "iopub.status.busy": "2023-12-29T12:41:43.816039Z",
     "iopub.status.idle": "2023-12-29T12:41:43.821576Z",
     "shell.execute_reply": "2023-12-29T12:41:43.820546Z",
     "shell.execute_reply.started": "2023-12-29T12:41:43.816202Z"
    },
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def train_batch(model, image, target):\n",
    "  \n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    return output, loss\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "def test_batch(model, image, target):\n",
    "\n",
    "    output = model(image)\n",
    "    loss = criterion(output, target)\n",
    "\n",
    "    return output, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "190868f8ab282cb5",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-12-27T03:02:23.715241Z"
    },
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:43.822489Z",
     "iopub.status.busy": "2023-12-29T12:41:43.822343Z",
     "iopub.status.idle": "2023-12-29T12:41:43.827954Z",
     "shell.execute_reply": "2023-12-29T12:41:43.827012Z",
     "shell.execute_reply.started": "2023-12-29T12:41:43.822472Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# training_loss = []\n",
    "# training_acc = []\n",
    "# testing_loss = []\n",
    "# testing_acc = []\n",
    "# start_time = time.time()  # 记录开始时间\n",
    "# all_preds = []\n",
    "# all_targets = []\n",
    "# # 日志文件\n",
    "# log_file = 'training_log.txt'\n",
    "# for epoch in tqdm(range(NUM_EPOCHS)):\n",
    "#     model.train()\n",
    "#     torch.cuda.empty_cache()\n",
    "\n",
    "#     ##########################\n",
    "#     ### Training\n",
    "#     ##########################\n",
    "\n",
    "#     running_cls_loss = 0.0\n",
    "#     running_cls_corrects = 0\n",
    "\n",
    "#     with tqdm(enumerate(train_dataloader), total=len(train_dataloader)) as t:\n",
    "#         for batch_idx, (image, target) in t:\n",
    "\n",
    "#             image = image.to(device)\n",
    "#             target = target.to(device)\n",
    "#             #######################\n",
    "#             # 为使用L1loss function：\n",
    "#             # target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "    \n",
    "#             # train model\n",
    "#             # outputs, loss = train_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "    \n",
    "#             outputs, loss = train_batch(model, image, target)\n",
    "#             #########################\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "#             loss_data = loss.data.item()\n",
    "#             if np.isnan(loss_data):\n",
    "#                 raise ValueError('loss is nan while training')\n",
    "#             running_cls_loss += loss.item()\n",
    "#             running_cls_corrects += torch.sum(preds == target.data)\n",
    "    \n",
    "#             loss.backward()\n",
    "#             optimizer.step()\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#     epoch_loss = running_cls_loss / len(train_set)\n",
    "#     epoch_acc = running_cls_corrects.double() / len(train_set)\n",
    "\n",
    "#     print(f'Epoch: {epoch + 1}/{NUM_EPOCHS} Train Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "#     #########\n",
    "#     all_preds.extend(preds.cpu().numpy())\n",
    "#     all_targets.extend(target.cpu().numpy())\n",
    "#     f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "#     print(f\"f1 score in {epoch}th epoch is {f1}\")\n",
    "\n",
    "#     ############\n",
    "#     training_loss.append(epoch_loss)\n",
    "#     training_acc.append(epoch_acc.cpu().detach().numpy())\n",
    "\n",
    "#     # change learning rate\n",
    "#     scheduler.step()\n",
    "\n",
    "#     ##########################\n",
    "#     ### Testing\n",
    "#     ##########################\n",
    "#     # # eval model during training or in the last epoch\n",
    "\n",
    "#     if (epoch + 1) % EVAL_INTERVAL == 0 or (epoch + 1) == NUM_EPOCHS:\n",
    "#         print('Begin test......')\n",
    "#         model.eval()\n",
    "\n",
    "#         val_loss = 0.0\n",
    "#         val_corrects = 0\n",
    "\n",
    "#         with tqdm(enumerate(test_dataloader), total=len(test_dataloader)) as t:\n",
    "#             for batch_idx, (image, target) in t:\n",
    "#                 image = image.to(device)\n",
    "#                 target = target.to(device)\n",
    "    \n",
    "#                 # test model\n",
    "#                 ##################\n",
    "#                 #  target_one_hot = one_hot_encoding(target, 10)  # 转换成 one-hot 编码\n",
    "#                 outputs, loss = test_batch(model, image, target)\n",
    "#                 #  outputs, loss = test_batch(model, image, target_one_hot)  # 使用 one-hot 编码后的目标\n",
    "#                 #######################\n",
    "#                 _, preds = torch.max(outputs, 1)\n",
    "    \n",
    "#                 val_loss += loss.item()\n",
    "#                 val_corrects += torch.sum(preds == target.data)\n",
    "\n",
    "#         val_loss = val_loss / len(test_set)\n",
    "#         val_acc = val_corrects.double() / len(test_set)\n",
    "#         print(f'Test Loss: {val_loss:.4f} Acc: {val_acc:.4f}')\n",
    "#         testing_loss.append(val_loss)\n",
    "#         testing_acc.append(val_acc.cpu().detach().numpy())\n",
    "#     with open(log_file, 'a') as f:\n",
    "#         f.write(f'Epoch {epoch + 1}/{NUM_EPOCHS}\\n')\n",
    "#         f.write(f'Train Accuracy: {epoch_acc:.4f}%\\n')\n",
    "#         f.write(f'Test Accuracy: {val_acc:.4f}%\\n')\n",
    "#         f.write('\\n')\n",
    "\n",
    "#         # save the model in last epoch\n",
    "#         if (epoch + 1) == NUM_EPOCHS:\n",
    "\n",
    "#             state = {\n",
    "#                 'state_dict': model.state_dict(),\n",
    "#                 'acc': epoch_acc,\n",
    "#                 'epoch': (epoch + 1),\n",
    "#             }\n",
    "\n",
    "#             # check the dir\n",
    "#             if not os.path.exists(SAVE_DIR):\n",
    "#                 os.makedirs(SAVE_DIR)\n",
    "\n",
    "#             # save the state\n",
    "#             torch.save(state, osp.join(SAVE_DIR, 'checkpoint_%s.pth' % (str(epoch + 1))))\n",
    "\n",
    "# end_time = time.time()  # 记录结束时间\n",
    "# duration = end_time - start_time  # 计算训练时间\n",
    "# print(f\"训练模型用时：{duration}秒\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2b96f1b5b82cb40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:43.828747Z",
     "iopub.status.busy": "2023-12-29T12:41:43.828577Z",
     "iopub.status.idle": "2023-12-29T12:41:43.832764Z",
     "shell.execute_reply": "2023-12-29T12:41:43.831904Z",
     "shell.execute_reply.started": "2023-12-29T12:41:43.828731Z"
    },
    "is_executing": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "# inputs, classes = next(iter(test_dataloader))\n",
    "# input = inputs[0]\n",
    "\n",
    "# ##################### Write your answer here ##################\n",
    "# # input: image, model\n",
    "# # outputs: predict_label, probabilities\n",
    "# # predict_label is the index (or label) of the class with the highest probability from the probabilities.\n",
    "# ###############################################################\n",
    "# input_tensor = input.unsqueeze(0).to(device)\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     output = model(input_tensor)\n",
    "#     probabilities = F.softmax(output, dim=1)\n",
    "#     predict_label = torch.argmax(probabilities)\n",
    "\n",
    "# predicted_class = class_names[predict_label.item()]\n",
    "# predicted_probability = probabilities[0][predict_label.item()]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # image = input.numpy().transpose((1, 2, 0))\n",
    "# # plt.imshow(image)\n",
    "# # plt.text(17, 30, f'Predicted Class: {predicted_class}\\nProbability: {predicted_probability:.2f}',\n",
    "# #             color='white', backgroundcolor='black', fontsize=8)\n",
    "# # plt.show()\n",
    "\n",
    "# # Print probabilities for each class\n",
    "# print('Print probabilities for each class:')\n",
    "# for i in range(len(class_names)):\n",
    "#     print(f'{class_names[i]}: {probabilities[0][i].item():.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fc9d42-1ddf-45f7-83f9-bfa409f83694",
   "metadata": {},
   "source": [
    "在共形预测中，覆盖率（Coverage Rate） 和 平均集大小（Average Set Size） 是两个重要的评估指标：\n",
    "\n",
    "覆盖率：衡量预测集合中正确包含真实标签的频率。高覆盖率表明预测集合通常包含真实的标签。\n",
    "\n",
    "平均集大小：表示平均每次预测时预测集合中包含的标签数量。较小的平均集大小通常表示预测具有较高的确定性。\n",
    "\n",
    "这些指标帮助评估共形预测模型的效果，特别是在不确定性和可靠性方面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c2a826427bf81b40",
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:43.833516Z",
     "iopub.status.busy": "2023-12-29T12:41:43.833381Z",
     "iopub.status.idle": "2023-12-29T12:41:46.274334Z",
     "shell.execute_reply": "2023-12-29T12:41:46.273454Z",
     "shell.execute_reply.started": "2023-12-29T12:41:43.833500Z"
    },
    "is_executing": true,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting torchcp\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/08/38/2f3468393a336720de2db7a08b1a70ec762e8741d46dad9e24762ff77cce/torchcp-0.1.2-py3-none-any.whl (54 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m890.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: torchcp\n",
      "Successfully installed torchcp-0.1.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install torchcp\n",
    "from torchcp.classification.scores import THR, APS, SAPS, RAPS\n",
    "from torchcp.classification.predictors import SplitPredictor, ClusterPredictor, ClassWisePredictor\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0da5130-be04-4d6f-8a6b-7c18bd9da3cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:46.275640Z",
     "iopub.status.busy": "2023-12-29T12:41:46.275467Z",
     "iopub.status.idle": "2023-12-29T12:41:46.282462Z",
     "shell.execute_reply": "2023-12-29T12:41:46.281624Z",
     "shell.execute_reply.started": "2023-12-29T12:41:46.275619Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #!pip install torchcp\n",
    "\n",
    "# log_name=\"CIFAR10\"\n",
    "# model=torch.load(\"CIFAR10model\")\n",
    "# results = []\n",
    "# alphas = [0.05, 0.1, 0.15, 0.2]\n",
    "# saps_weights = [0.5, 1, 1.5, 2]\n",
    "# raps_penalties = [0.05, 0.1, 0.15, 0.2]\n",
    "# raps_kregs = [0, 1, 2, 5]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "\n",
    "# all_predict_sets_list=[]\n",
    "# for alpha in alphas:\n",
    "# #     for weight in saps_weights:\n",
    "# #         for penalty in raps_penalties:\n",
    "# #             for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"alpha loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     #print(result)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result+\"\\n\")\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     #print(all_predict_set)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(' '.join(all_predict_set)+\"\\n\")\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "# results = []\n",
    "# all_predict_sets_list=[]\n",
    "# # for alpha in alphas:\n",
    "# #     for weight in saps_weights:\n",
    "# #         for penalty in raps_penalties:\n",
    "# for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"kreg loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     #print(result)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result+\"\\n\")\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     #print(all_predict_set)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(' '.join(all_predict_set)+\"\\n\")\n",
    "\n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "# results = []\n",
    "# all_predict_sets_list=[]\n",
    "# # for alpha in alphas:\n",
    "# for weight in saps_weights:\n",
    "# #         for penalty in raps_penalties:\n",
    "# #             for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"weight loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     #print(result)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result+\"\\n\")\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     #print(all_predict_set)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(' '.join(all_predict_set)+\"\\n\")\n",
    "        \n",
    "        \n",
    "\n",
    "# alpha=0.1\n",
    "# weight=1\n",
    "# penalty=0.1\n",
    "# kreg=0\n",
    "# results = []\n",
    "# all_predict_sets_list=[]\n",
    "# # for alpha in alphas:\n",
    "# #     for weight in saps_weights:\n",
    "# for penalty in raps_penalties:\n",
    "# #             for kreg in raps_kregs:\n",
    "#                 score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "#                 predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "#                 for score_function in score_functions:\n",
    "#                     for Predictor in predictors:\n",
    "#                         predictor = Predictor(score_function=score_function, model=model)\n",
    "#                         predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "#                         all_predict_sets = []\n",
    "#                         for batch in test_dataloader:\n",
    "#                             images, _ = batch\n",
    "#                             images = images.to(device)\n",
    "#                             predict_set = predictor.predict(images)\n",
    "#                             all_predict_sets.extend(predict_set)\n",
    "#                         all_predict_sets_list.append(all_predict_sets)\n",
    "#                         result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "#                         result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "#                                       f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "#                                       f\"Predictor: {Predictor.__name__}, \"\n",
    "#                                       f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "#                                       f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "#                                       f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "#                                       f\"Average Size: {result_dict['Average_size']}\")\n",
    "#                         results.append(result_str)\n",
    "\n",
    "# # 打印或处理存储的结果\n",
    "# with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#     file.write(\"penalty loop\"+\"\\n\")\n",
    "# for result in results:\n",
    "#     #print(result)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(result+\"\\n\")\n",
    "# for all_predict_set in all_predict_sets_list:\n",
    "#     #print(all_predict_set)\n",
    "#     with open(f'12.27result_{log_name}.txt', 'a') as file:\n",
    "#         file.write(' '.join(all_predict_set)+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7eaff726-9247-44fd-91e5-2001c5a6f51f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:46.283251Z",
     "iopub.status.busy": "2023-12-29T12:41:46.283103Z",
     "iopub.status.idle": "2023-12-29T12:41:46.289983Z",
     "shell.execute_reply": "2023-12-29T12:41:46.289259Z",
     "shell.execute_reply.started": "2023-12-29T12:41:46.283235Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "def evaluate_and_log_results(model, alphas, weights, penalties, kregs, train_dataloader, test_dataloader, device, log_name):\n",
    "    results = []\n",
    "    all_predict_sets_list = []\n",
    "\n",
    "    for alpha in alphas:\n",
    "        for weight in weights:\n",
    "            for penalty in penalties:\n",
    "                for kreg in kregs:\n",
    "                    score_functions = [THR(), APS(), SAPS(weight=weight), RAPS(penalty=penalty, kreg=kreg)]\n",
    "                    predictors = [SplitPredictor, ClusterPredictor, ClassWisePredictor]\n",
    "\n",
    "                    for score_function in score_functions:\n",
    "                        for Predictor in predictors:\n",
    "                            predictor = Predictor(score_function=score_function, model=model)\n",
    "                            predictor.calibrate(train_dataloader, alpha=alpha)\n",
    "\n",
    "                            all_predict_sets = []\n",
    "                            for batch in test_dataloader:\n",
    "                                images, _ = batch\n",
    "                                images = images.to(device)\n",
    "                                predict_set = predictor.predict(images)\n",
    "                                all_predict_sets.extend(predict_set)\n",
    "                            all_predict_sets_list.append(all_predict_sets)\n",
    "                            result_dict = predictor.evaluate(test_dataloader)\n",
    "\n",
    "                            result_str = (f\"Dataset: {type(train_set).__name__}, \"\n",
    "                                          f\"Score Function: {score_function.__class__.__name__}, \"\n",
    "                                          f\"Predictor: {Predictor.__name__}, \"\n",
    "                                          f\"Alpha: {alpha}, Weight: {weight if isinstance(score_function, SAPS) else 'N/A'}, \"\n",
    "                                          f\"Penalty: {penalty if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "                                          f\"kreg: {kreg if isinstance(score_function, RAPS) else 'N/A'}, \"\n",
    "                                          f\"Coverage Rate: {result_dict['Coverage_rate']}, \"\n",
    "                                          f\"Average Size: {result_dict['Average_size']}\")\n",
    "                            results.append(result_str)\n",
    "\n",
    "    # Write results to file\n",
    "    with open(f'{log_name}_results_new.txt', 'a') as file:\n",
    "        for result in results:\n",
    "            file.write(\"****************\")\n",
    "            file.write(result + \"\\n\")\n",
    "        for all_predict_set in all_predict_sets_list:\n",
    "            predict_set_str = [' '.join(map(str, sublist)) for sublist in all_predict_set]\n",
    "            file.write('\\n'.join(predict_set_str) + \"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac128a1-8c49-4d9f-977a-9c972c669c9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-29T12:41:46.290676Z",
     "iopub.status.busy": "2023-12-29T12:41:46.290529Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "alphas = [0.05, 0.1, 0.15, 0.2]\n",
    "saps_weights = [0.5, 1, 1.5, 2]\n",
    "raps_penalties = [0.05, 0.1, 0.15, 0.2]\n",
    "raps_kregs = [0, 1, 2, 5]\n",
    "log_name=\"CIFAR10\"\n",
    "model=torch.load(\"CIFAR10model\")\n",
    "evaluate_and_log_results(model, [0.1], [1], [0.1], raps_kregs, train_dataloader, test_dataloader, device, log_name)\n",
    "evaluate_and_log_results(model, [0.1], [1], raps_penalties, [0], train_dataloader, test_dataloader, device, log_name)\n",
    "evaluate_and_log_results(model, [0.1], saps_weights, [0.1], [0], train_dataloader, test_dataloader, device, log_name)\n",
    "evaluate_and_log_results(model, alphas, [1], [0.1], [0], train_dataloader, test_dataloader, device, log_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239127a6-c088-454a-8b03-e93d03ecff7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"**\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86de0dad-cd46-46ec-b0b4-2fb348adfc16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#print(all_predict_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9114219-b2f2-40b6-a612-8003a2bb33e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model,\"CIFAR10model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea215218-bbdf-4540-b263-7fe41e823cd1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
